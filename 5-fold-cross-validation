
library(plyr)
library(caret)
library(haven)
library(ROCR)   

---------------------------------------------------------------------------------------
#import data, including dependent and independent
bank_logit=read_sav("C:\\Users\\mjdee\\Desktop\\_DATA_2021-07-20_09(8.05)v3.0(1).sav")
bank_logit1=bank_logit[,c(8:15)]
bank_logit1=bank_logit1[complete.cases(bank_logit1),]

#set 5 folds
folds <- createFolds(y=bank_logit1$death_whether,k=5)
length(folds)
print("***组号***")

---------------------------------------------------------------------------------------
 #create a blank dataframe to contain the result name tabledata
 #name are listed
tabledata=as.data.frame(matrix(nrow=6,ncol=8))
names(tabledata)=c("fold","Cox-Snell R2","Nagelkerke R2","accuracy","precision","recall","F_measure","AUC");

#create 5 loops to calculate 5 models using 5 samples
for(i in 1:5){ 
  tabledata[i,1]=i;
  
train_cv <- bank_logit1[-folds[[i]],]
test_cv <- bank_logit1[folds[[i]],]


pre <- glm(death_whether ~.,family=binomial(link = "logit"),data = train_cv)

#real= the real outcome in test set
real <- test_cv$death_whether

#result of predict is predict.pre, using 'predict' function: the established model is pre,newdata is test set,type is 'response'
predict.pre <- predict(pre,type='response',newdata=test_cv)

#0.5 is the cutoff, >0.5 returns 1,else return 0
predict =ifelse(predict.pre>0.5,1,0)

#put predict back into test set
test_cv$predict = predict

#create a dataframe contain the real outcome and the predict
res <- data.frame(real,predict)
n = nrow(test_cv)      

#2th column: Cox-Snell
tabledata[i,2] <- 1-exp((pre$deviance-pre$null.deviance)/n)

#3th column:Nagelkerke (Nagelkerke is a scaled Cox-Snell)
R2=as.numeric(tabledata[i,2])
tabledata[i,3] <-R2/(1-exp((-pre$null.deviance)/n))  
#cat("Nagelkerke R2=",R2,"\n")

##other cofficients
#residuals(pre)     
#coefficients(pre) 
#anova(pre)        


true_value=test_cv[,"death_whether"]
predict_value=test_cv[,"predict"]

#4th colunm:accuracy
error = predict_value-true_value
tabledata[i,4]= (nrow(test_cv)-sum(abs(error)))/nrow(test_cv) #精确度--判断正确的数量占总数的比例

#计算Precision，Recall和F-measure
#一般来说，Precision就是检索出来的条目（比如：文档、网页等）有多少是准确的，Recall就是所有准确的条目有多少被检索出来了
#和混淆矩阵结合，Precision计算的是所有被检索到的item（TP+FP）中,"应该被检索到的item（TP）”占的比例；Recall计算的是所有检索到的item（TP）占所有"应该被检索到的item（TP+FN）"的比例。
tabledata[i,5]=sum(true_value & predict_value)/sum(predict_value)  #真实值预测值全为1 / 预测值全为1 --- 提取出的正确信息条数/提取出的信息条数
tabledata[i,6]=sum(predict_value & true_value)/sum(true_value)  #真实值预测值全为1 / 真实值全为1 --- 提取出的正确信息条数 /样本中的信息条数
#P和R指标有时候会出现的矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure（又称为F-Score）
precision=as.numeric(tabledata[i,5])
recall=as.numeric(tabledata[i,6])
tabledata[i,7]=2*precision*recall/(precision+recall)   #F-Measure是Precision和Recall加权调和平均，是一个综合评价指标
      
#ROC曲线
pred <- prediction(predict.pre,true_value)   #预测值(0.5二分类之前的预测值)和真实值   
tabledata[i,8]=performance(pred,'auc')@y.values        #AUC值
}
tabledata[6,1]="mean"
tabledata[,1:8] <- lapply(tabledata[,1:8], as.numeric)
tabledata[6,]= colMeans(tabledata[1:5,],dims = 1)

write.csv(tabledata,"C:/Users/mjdee/Desktop/fold5.csv",row.names = F)
